set.seed(1)
c.tree = rpart(Price ~ ., data = train, method = "class", cp=0, minbucket=1, minsplit = 1, xval =5)
library(forecast)
library(caret)
library(Metrics)
library(dplyr)
library(reshape2)
library(scales)
library(rpart)
set.seed(1)
c.tree = rpart(Price ~ ., data = train, method = "class", cp=0, minbucket=1, minsplit = 1, xval =5)
library(forecast)
library(caret)
library(Metrics)
library(dplyr)
library(reshape2)
library(scales)
library(rpart)
set.seed(1)
c.tree = rpart(Output ~ ., data = train, method = "class", cp=0, minbucket=1, minsplit = 1, xval =5)
length(c.tree$frame$var[c.tree$frame$var == "<leaf>"])
library(forecast)
library(caret)
library(Metrics)
library(dplyr)
library(reshape2)
library(scales)
library(rpart)
set.seed(1)
full_CT = rpart(Output ~ ., data = train, method = "class", cp=0, minbucket=1, minsplit = 1, xval =5)
length(full_CT$frame$var[full_CT$frame$var == "<leaf>"])
full_CT
rpart.plot(full_CT, type = 1, extra = 1, under = TRUE, split.font = 1, varlen = -10)
library(ggplot2)
library(cowplot)
install.packages("cowplot")
library(ggplot2)
library(cowplot)
rpart.plot(full_CT, type = 1, extra = 1, under = TRUE, split.font = 1, varlen = -10)
library(ggplot2)
library(cowplot)
library(rpart.plot)
install.packages("library(rpart.plot)")
library(ggplot2)
library(cowplot)
library(rpart.plot)
library(ggplot2)
library(cowplot)
library(rpart.plot)
install.packages("rpart.plot")
library(ggplot2)
library(cowplot)
library(rpart.plot)
rpart.plot(full_CT, type = 1, extra = 1, under = TRUE, split.font = 1, varlen = -10)
library(forecast)
library(caret)
library(Metrics)
library(dplyr)
library(reshape2)
library(scales)
library(rpart)
set.seed(1)
full_CT = rpart(Output ~ ., data = train, method = "class", cp=0, minbucket=1, minsplit = 1, xval =5)
length(full_CT$frame$var[full_CT$frame$var == "<leaf>"])
row_minxerr=which.min(full_CT$cptable[,4])
cp_minxerr=full_CT$cptable[row_minxerr,1]
cp_minxerr
minxerr=full_CT$cptable[row_minxerr,4]
xstd_minxerr=full_CT$cptable[row_minxerr,5]
cp_best_pruned_CT=minxerr+xstd_minxerr
cp_best_pruned_CT
printcp(full_CT)[,4]
cp_xerr=full_CT$cptable[5,1]
cp_xerr
plotcp(full_CT) +
abline(v = row_minxerr, lty = "dashed")+
abline(v = 5, lty = "dotted")
pruned_CT <- prune(full_CT, cp = cp_minxerr)
length(pruned_CT$frame$var[pruned_CT$frame$var == "<leaf>"])
rpart.plot(pruned_CT, type = 1, extra = 1, under = TRUE, split.font = 1, varlen = -10)
best_pruned_CT <- prune(full_CT,cp = cp_xerr)
length(best_pruned_CT$frame$var[best_pruned_CT$frame$var == "<leaf>"])
rpart.plot(best_pruned_CT, type = 1, extra = 1, under = TRUE, split.font = 1, varlen = -10)
View(valid)
pred_valid_pruned_CT= predict(pruned_CT, valid, type="class")
valid$Personal.Loan=as.factor(valid$Personal.Loan)
pred_valid_pruned_CT= predict(pruned_CT, valid, type="class")
valid$Output=as.factor(valid$Output)
CT_cm_valid=confusionMatrix(pred_valid_pruned_CT, valid$Output , positive = "1")
CT_cm_valid
fourfoldplot(confusionMatrix(pred_valid_pruned_CT, class_valid.df$Output , positive = "1")$table)
fourfoldplot(confusionMatrix(pred_valid_pruned_CT, valid$Output , positive = "1")$table)
pred_test_pruned_CT= predict(pruned_CT, test, type="class")
test$Output=as.factor(test$Output)
CT_cm_valid=confusionMatrix(pred_test_pruned_CT, test$Output , positive = "1")
CT_cm_valid
fourfoldplot(confusionMatrix(pred_test_pruned_CT, test$Output , positive = "1")$table)
knitr::opts_chunk$set(echo = TRUE)
df <- read.csv(file = "C:/Users/leomi/Downloads/Data-mining/Project_CVTDM/onlinedeliverydata.csv", header = T, sep = ",")
#setwd("C:/Users/andre/Desktop/Creating Value Project")
#df <- read.csv("onlinedeliverydata.csv", header = TRUE, sep = ",")
df <- df[,-c(8,9,10)]
str(df)
attach(df)
library(dplyr)
df1 <- df[,-52]
for (i in c(14:33,37:41)) {
df1[,i] <- recode(df1[,i], "Strongly Agree"=5, "Agree"=4,"Neutral"=3,"Disagree"=2,"Strongly Disagree"=1)
}
library(naniar)
gg_miss_var(df1[,c(14:33,37:41)], show_pct = TRUE)
levels(df[,51])
df1$Output <- recode(df1$Output, "Yes"=1, "No"=0)
gg_miss_var(df[,50:51], show_pct = TRUE)
levels(as.factor(df1[,5]))
df1$Monthly.Income=recode(df$Monthly.Income, "10001 to 25000"=17500.5, "25001 to 50000"=37500, "Below Rs.10000" = 10000, "More than 50000"=50000, "No Income"=0 )
levels(df[,43])
str(df[,43:50])
for (i in c(43:50)) {
df1[,i] <- recode(df1[,i], "Important"=4, "Moderately Important"=3, "Slightly Important"=2, "Unimportant"=1, "Very Important"=5)
}
gg_miss_var(df1[,43:50], show_pct = TRUE)
levels(df[,34])
str(df[,c(34,42)])
for (i in c(34,42)) {
df1[,i] <- recode(df1[,i], "Maybe"=2, "No"=1, "Yes"=3)
}
gg_miss_var(df1[,c(34,42)], show_pct = TRUE)
levels(df[,2])
df1$Gender <- recode(df1$Gender, "Female"=1, "Male"=0)
gg_miss_var(df1[,1:2], show_pct = TRUE)
df1=df1[,-c(8:13)]
str(df1)
df2 <- df1[,c(1,3,4,5,6,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,27,30,36,38,39,40,41,45)]
str(df2)
df2$Marital.Status =as.factor(df2$Marital.Status)
df2$Occupation=as.factor(df2$Occupation)
df2$Educational.Qualifications=as.factor(df2$Educational.Qualifications)
library(caret)
dummies=dummyVars(~., data=df2)
df.dummy=as.data.frame(predict(dummies, newdata=df2))
df.dummy=df.dummy[,-c(3,6,14)]
str(df.dummy)
df.dummy$Output <- as.factor(df.dummy$Output)
df2$Output <- as.factor(df2$Output)
library(caret)
set.seed(1)
train_rows <- createDataPartition(df2$Output, p = .5, list = FALSE)
train <- df2[train_rows,]
valid_test <- df2[-train_rows,]
set.seed(1)
valid_rows <- createDataPartition(valid_test$Output, p = .6, list = FALSE)
valid <- valid_test[valid_rows,]
test <- valid_test[-valid_rows,]
set.seed(1)
train_rows1 <- createDataPartition(df.dummy$Output, p = .5, list = FALSE)
train.dummy <- df.dummy[train_rows1,]
valid_test.dummy <- df.dummy[-train_rows1,]
set.seed(1)
valid_rows1 <- createDataPartition(valid_test.dummy$Output, p = .6, list = FALSE)
valid.dummy <- valid_test.dummy[valid_rows1,]
test.dummy <- valid_test.dummy[-valid_rows1,]
print(table(train$Output) / table(df2$Output)) #Training set partition
print(table(valid$Output) / table(df2$Output)) #Validation set partition
print(table(test$Output) / table(df2$Output)) #Test set partition
library(forecast)
library(caret)
library(Metrics)
library(dplyr)
library(reshape2)
library(scales)
library(rpart)
set.seed(1)
full_CT = rpart(Output ~ ., data = train, method = "class", cp=0, minbucket=1, minsplit = 1, xval =5)
length(full_CT$frame$var[full_CT$frame$var == "<leaf>"])
library(ggplot2)
library(cowplot)
library(rpart.plot)
rpart.plot(full_CT, type = 1, extra = 1, under = TRUE, split.font = 1, varlen = -10)
row_minxerr=which.min(full_CT$cptable[,4])
cp_minxerr=full_CT$cptable[row_minxerr,1]
cp_minxerr
minxerr=full_CT$cptable[row_minxerr,4]
xstd_minxerr=full_CT$cptable[row_minxerr,5]
cp_best_pruned_CT=minxerr+xstd_minxerr
cp_best_pruned_CT
printcp(full_CT)[,4]
cp_xerr=full_CT$cptable[5,1]
cp_xerr
plotcp(full_CT) +
abline(v = row_minxerr, lty = "dashed")+
abline(v = 5, lty = "dotted")
pruned_CT <- prune(full_CT, cp = cp_minxerr)
length(pruned_CT$frame$var[pruned_CT$frame$var == "<leaf>"])
rpart.plot(pruned_CT, type = 1, extra = 1, under = TRUE, split.font = 1, varlen = -10)
pred_valid_pruned_CT= predict(pruned_CT, valid, type="class")
valid$Output=as.factor(valid$Output)
CT_cm_valid=confusionMatrix(pred_valid_pruned_CT, valid$Output , positive = "1")
CT_cm_valid
fourfoldplot(confusionMatrix(pred_valid_pruned_CT, valid$Output , positive = "1")$table)
pred_test_pruned_CT= predict(pruned_CT, test, type="class")
test$Output=as.factor(test$Output)
CT_cm_valid=confusionMatrix(pred_test_pruned_CT, test$Output , positive = "1")
CT_cm_valid
fourfoldplot(confusionMatrix(pred_test_pruned_CT, test$Output , positive = "1")$table)
best_pruned_CT <- prune(full_CT,cp = cp_xerr)
length(best_pruned_CT$frame$var[best_pruned_CT$frame$var == "<leaf>"])
rpart.plot(best_pruned_CT, type = 1, extra = 1, under = TRUE, split.font = 1, varlen = -10)
pred_valid_best_pruned_CT= predict(best_pruned_CT, valid, type="class")
valid$Output=as.factor(valid$Output)
CT_cm_valid=confusionMatrix(pred_valid_best_pruned_CT, valid$Output , positive = "1")
CT_cm_valid
fourfoldplot(confusionMatrix(pred_valid_best_pruned_CT, valid$Output , positive = "1")$table)
pred_test_best_pruned_CT= predict(best_pruned_CT, test, type="class")
test$Output=as.factor(test$Output)
CT_cm_valid=confusionMatrix(pred_test_best_pruned_CT, test$Output , positive = "1")
CT_cm_valid
fourfoldplot(confusionMatrix(pred_test_best_pruned_CT, test$Output , positive = "1")$table)
set.seed(1)
pruned_CT <- prune(full_CT, cp = cp_minxerr)
length(pruned_CT$frame$var[pruned_CT$frame$var == "<leaf>"])
set.seed(1)
boost <- boosting(Output ~ ., data = train)
library(cowplot)
library(caret)
library(rpart)
library(gains)
install.packages("gains")
install.packages("adabag")
install.packages("randomForest")
library(cowplot)
library(caret)
library(rpart)
library(gains)
library(adabag)
library(randomForest)
library(reshape2)
library(cowplot)
library(caret)
library(rpart)
library(gains)
library(adabag)
library(randomForest)
library(reshape2)
set.seed(1)
boost <- boosting(Output ~ ., data = train)
pred_boost_valid <- predict(boost, valid)
boost_valid.cm = confusionMatrix(as.factor(pred_boost_valid$class), validf$Output)
pred_boost_valid <- predict(boost, valid)
boost_valid.cm = confusionMatrix(as.factor(pred_boost_valid$class), valid$Output)
boost_valid.cm
pred_boost_test <- predict(boost, test)
boost_test.cm = confusionMatrix(as.factor(pred_boost_test$class), validf$Output)
pred_boost_test <- predict(boost, test)
boost_test.cm = confusionMatrix(as.factor(pred_boost_test$class), valid$Output)
pred_boost_test <- predict(boost, test)
boost_test.cm = confusionMatrix(as.factor(pred_boost_test$class), as.factor(valid$Output))
pred_boost_test <- predict(boost, test)
boost_test.cm = confusionMatrix((pred_boost_test$class), (valid$Output))
pred_boost_test <- predict(boost, test)
boost_test.cm = confusionMatrix(as.factor(pred_boost_test$class), (valid$Output))
pred_boost_test$class
valid$Output
pred_boost_test$class
pred_boost_test <- predict(boost, test)
boost_test.cm = confusionMatrix(as.factor(pred_boost_test$class), (test$Output))
boost_test.cm
set.seed(1)
bagt <- bagging(Output ~ ., data = train)
pred <- predict(bagt, valid)
bag_valid.cm = confusionMatrix(as.factor(pred$class), valid$Output)
bag_valid.cm
pred <- predict(bagt, test)
bag_test.cm = confusionMatrix(as.factor(pred$class), test$Output)
bag_test.cm
set.seed(1)
rf <- randomForest(Output ~ ., data = train, mtry=4, importance = T)
pred <- predict(rf, valid)
rf_valid.cm = confusionMatrix(as.factor(pred), valid$Output)
rf_valid.cm
pred <- predict(rf, test)
rf_valid.cm = confusionMatrix(as.factor(pred), test$Output)
rf_valid.cm
pred_boost_valid <- predict(boost, valid)
boost_valid.cm = confusionMatrix(as.factor(pred_boost_valid$class), valid$Output, positive = "1")
boost_valid.cm
pred_boost_test <- predict(boost, test)
boost_test.cm = confusionMatrix(as.factor(pred_boost_test$class), (test$Output),positive = "1")
boost_test.cm
pred <- predict(bagt, valid)
bag_valid.cm = confusionMatrix(as.factor(pred$class), valid$Output,positive = "1")
bag_valid.cm
pred <- predict(bagt, test)
bag_test.cm = confusionMatrix(as.factor(pred$class), test$Output,positive = "1")
bag_test.cm
pred <- predict(rf, valid)
rf_valid.cm = confusionMatrix(as.factor(pred), valid$Output,positive = "1")
rf_valid.cm
pred <- predict(rf, test)
rf_valid.cm = confusionMatrix(as.factor(pred), test$Output, positive = "1")
rf_valid.cm
library(MASS)
library(DiscriMiner)
library(MASS)
library(DiscriMiner)
View(train)
da1.reg <- linDA(train[,1:30], train[,31])
model1 <- lda(Output~., data = train)
model1
pred.valid1=predict(model1,valid[,-31])
cm.da.valid1=confusionMatrix(as.factor(pred.valid1$class),as.factor(valid$Output) ,positive = "1")
cm.da.valid1
da.pred.test=predict(model1,test[,-31])
cm.da.test=confusionMatrix(as.factor(da.pred.test$class),as.factor(test$Output) ,positive = "1")
cm.da.test
fourfoldplot(cm.da.valid1$table)
fourfoldplot(cm.da.test$table)
pred.valid=predict(model1,valid[,-31])
cm.da.valid=confusionMatrix(as.factor(pred.valid$class),as.factor(valid$Output) ,positive = "1")
cm.da.valid
knitr::opts_chunk$set(echo = TRUE)
df <- read.csv(file = "C:/Users/leomi/Downloads/Data-mining/Project_CVTDM/onlinedeliverydata.csv", header = T, sep = ",")
#df <- read.csv(file = "C:/Users/leomi/Downloads/Data-mining/Project_CVTDM/onlinedeliverydata.csv", header = T, sep = ",")
setwd("C:/Users/andre/Desktop/Creating Value Project")
df <- read.csv("onlinedeliverydata.csv", header = TRUE, sep = ",")
df <- df[,-c(8,9,10)]
str(df)
attach(df)
knitr::opts_chunk$set(echo = TRUE)
#df <- read.csv(file = "C:/Users/leomi/Downloads/Data-mining/Project_CVTDM/onlinedeliverydata.csv", header = T, sep = ",")
setwd("C:/Users/andre/Desktop/Creating Value Project")
df <- read.csv("onlinedeliverydata.csv", header = TRUE, sep = ",")
df <- df[,-c(8,9,10)]
str(df)
attach(df)
library(dplyr)
df1 <- df[,-52]
for (i in c(14:33,37:41)) {
df1[,i] <- recode(df1[,i], "Strongly Agree"=5, "Agree"=4,"Neutral"=3,"Disagree"=2,"Strongly Disagree"=1)
}
library(naniar)
gg_miss_var(df1[,c(14:33,37:41)], show_pct = TRUE)
levels(df[,51])
df1$Output <- recode(df1$Output, "Yes"=1, "No"=0)
gg_miss_var(df[,50:51], show_pct = TRUE)
levels(as.factor(df1[,5]))
df1$Monthly.Income=recode(df$Monthly.Income, "10001 to 25000"=17500.5, "25001 to 50000"=37500, "Below Rs.10000" = 10000, "More than 50000"=50000, "No Income"=0 )
levels(df[,43])
str(df[,43:50])
for (i in c(43:50)) {
df1[,i] <- recode(df1[,i], "Important"=4, "Moderately Important"=3, "Slightly Important"=2, "Unimportant"=1, "Very Important"=5)
}
gg_miss_var(df1[,43:50], show_pct = TRUE)
levels(df[,34])
str(df[,c(34,42)])
for (i in c(34,42)) {
df1[,i] <- recode(df1[,i], "Maybe"=2, "No"=1, "Yes"=3)
}
gg_miss_var(df1[,c(34,42)], show_pct = TRUE)
levels(df[,2])
df1$Gender <- recode(df1$Gender, "Female"=1, "Male"=0)
gg_miss_var(df1[,1:2], show_pct = TRUE)
df1=df1[,-c(8:13)]
str(df1)
df2 = df1[,c(8,	9,	12,	10,	21,	11,	1	,30	,17,	14,	13,	39,	4,45)]
str(df2)
df2$Occupation=as.factor(df2$Occupation)
library(caret)
dummies=dummyVars(~., data=df2)
df.dummy=as.data.frame(predict(dummies, newdata=df2))
df.dummy=df.dummy[,-14]
str(df.dummy)
df.dummy$Output <- as.factor(df.dummy$Output)
df2$Output <- as.factor(df2$Output)
library(caret)
set.seed(1)
train_rows <- createDataPartition(df2$Output, p = .5, list = FALSE)
train <- df2[train_rows,]
valid_test <- df2[-train_rows,]
set.seed(1)
valid_rows <- createDataPartition(valid_test$Output, p = .6, list = FALSE)
valid <- valid_test[valid_rows,]
test <- valid_test[-valid_rows,]
set.seed(1)
train_rows1 <- createDataPartition(df.dummy$Output, p = .5, list = FALSE)
train.dummy <- df.dummy[train_rows1,]
valid_test.dummy <- df.dummy[-train_rows1,]
set.seed(1)
valid_rows1 <- createDataPartition(valid_test.dummy$Output, p = .6, list = FALSE)
valid.dummy <- valid_test.dummy[valid_rows1,]
test.dummy <- valid_test.dummy[-valid_rows1,]
print(table(train$Output) / table(df2$Output)) #Training set partition
print(table(valid$Output) / table(df2$Output)) #Validation set partition
print(table(test$Output) / table(df2$Output)) #Test set partition
library(MASS)
library(DiscriMiner)
#da1.reg <- linDA(train[,1:30], train[,31])
#da1.reg
#View(train)
model1 <- lda(Output~., data = train)
model1
pred.valid=predict(model1,valid[,-31])
cm.da.valid=confusionMatrix(as.factor(pred.valid$class),as.factor(valid$Output) ,positive = "1")
cm.da.valid
fourfoldplot(cm.da.valid$table)
da.pred.test=predict(model1,test[,-31])
cm.da.test=confusionMatrix(as.factor(da.pred.test$class),as.factor(test$Output) ,positive = "1")
cm.da.test
fourfoldplot(cm.da.test$table)
library(neuralnet)
library(caret)
library(nnet)
library(NeuralNetTools)
library(boot)
library(plyr)
library(ggplot2)
library(reshape)
train.dummy.scale=train.dummy
valid.dummy.scale=valid.dummy
test.dummy.scale=test.dummy
vars = c(1,7,12:36)
norm.values <- preProcess(train.dummy[, vars], method="range")
str(df.dummy)
train.st <- train.dummy
valid.st <- valid.dummy
test.st <- test.dummy
standard.values = preProcess(train.dummy[,c(1:12)], method=c("center", "scale"))
train.st[,c(1:12)] <- predict(standard.values, train.dummy[,c(1:12)])
valid.st[,c(1:12)] <- predict(standard.values, valid.dummy[,c(1:12)])
test.st[,c(1:12)] <- predict(standard.values, test.dummy[,c(1:12)])
str(train.st)
View(test.st)
library(FNN)
metrics <- data.frame(k=seq(1, 30, 1), balanced_accuracy = rep(0, 30), F1_score = rep(0, 30))
for(i in 1:30) {
knn <- knn(train=train.st[,-37], test=valid.st[,-37], cl=train.st[,37], k=i)
metrics[i, 2] = confusionMatrix(knn, valid.st[,37], positive = "1")$byClass["F1"]
metrics[i, 3] = confusionMatrix(knn, valid.st[,37], positive = "1")$byClass["F1"]
}
library(FNN)
metrics <- data.frame(k=seq(1, 30, 1), balanced_accuracy = rep(0, 30), F1_score = rep(0, 30))
for(i in 1:30) {
knn <- knn(train=train.st[,-16], test=valid.st[,-16], cl=train.st[,16], k=i)
metrics[i, 2] = confusionMatrix(knn, valid.st[,37], positive = "1")$byClass["F1"]
metrics[i, 3] = confusionMatrix(knn, valid.st[,37], positive = "1")$byClass["F1"]
}
library(FNN)
metrics <- data.frame(k=seq(1, 30, 1), balanced_accuracy = rep(0, 30), F1_score = rep(0, 30))
for(i in 1:30) {
knn <- knn(train=train.st[,-16], test=valid.st[,-16], cl=train.st[,16], k=i)
metrics[i, 2] = confusionMatrix(knn, valid.st[,16], positive = "1")$byClass["F1"]
metrics[i, 3] = confusionMatrix(knn, valid.st[,16], positive = "1")$byClass["F1"]
}
library(ggplot2)
ggplot(data= metrics)+
geom_line(aes(x=k, y=balanced_accuracy))+
theme_bw()
ggplot(data= metrics)+
geom_line(aes(x=k, y=F1_score))+
theme_bw()
which.max(metrics$balanced_accuracy)
which.max(metrics$F1_score)
knn1 <- knn(train=train.st[,-37], test=valid.st[,-37], cl=train.st[,37], k=1)
knn1 <- knn(train=train.st[,-16], test=valid.st[,-16], cl=train.st[,16], k=1)
#knn10 <- knn(train=train.st[,-37], test=valid.st[,-37], cl=train.st[,37], k=10)
knn1.test <- knn(train=train.st[,-16], test=test.st[,-16], cl=train.st[,16], k=1)
#knn10.test <- knn(train=train.st[,-37], test=test.st[,-37], cl=train.st[,37], k=10)
knn1.acc <- data.frame(df=c("valid","test"))
knn1.acc$Balanced_Accuracy <- c(confusionMatrix(knn1, valid.st[,16], positive = "1")$byClass["F1"], confusionMatrix(knn1.test, test.st[,16], positive = "1")$byClass["F1"])
knn1.acc$F1 <- c(confusionMatrix(knn1, valid.st[,16], positive = "1")$byClass["F1"], confusionMatrix(knn1.test, test.st[,16], positive = "1")$byClass["F1"])
# knn.acc <- data.frame(df=c("valid k=1","valid k=10","test k=1","test k=10"))
# knn.acc$Balanced_Accuracy <- c(confusionMatrix(knn1, valid.st[,37], positive = "1")$byClass["F1"], confusionMatrix(knn10, valid.st[,37], positive = "1")$byClass["F1"], confusionMatrix(knn1.test, test.st[,37], positive = "1")$byClass["F1"], confusionMatrix(knn10.test, test.st[,37], positive = "1")$byClass["F1"])
# knn.acc$F1 <- c(confusionMatrix(knn1, valid.st[,37], positive = "1")$byClass["F1"], confusionMatrix(knn10, valid.st[,37], positive = "1")$byClass["F1"], confusionMatrix(knn1.test, test.st[,37], positive = "1")$byClass["F1"], confusionMatrix(knn10.test, test.st[,37], positive = "1")$byClass["F1"])
library(reshape)
knn.melt <- melt(knn1.acc, id="df")
ggplot(data=knn.melt, aes(x=df, y=value, colour=variable))+
geom_col(position="dodge", fill="grey96")+
theme_bw()
View(knn1.acc)
library(FNN)
metrics <- data.frame(k=seq(1, 30, 1), balanced_accuracy = rep(0, 30), F1_score = rep(0, 30))
for(i in 1:30) {
knn <- knn(train=train.st[,-16], test=valid.st[,-16], cl=train.st[,16], k=i)
metrics[i, 2] = confusionMatrix(knn, valid.st[,16], positive = "1")$byClass["Balanced Accuracy"]
metrics[i, 3] = confusionMatrix(knn, valid.st[,16], positive = "1")$byClass["F1"]
}
library(ggplot2)
ggplot(data= metrics)+
geom_line(aes(x=k, y=balanced_accuracy))+
theme_bw()
ggplot(data= metrics)+
geom_line(aes(x=k, y=F1_score))+
theme_bw()
which.max(metrics$balanced_accuracy)
which.max(metrics$F1_score)
knn1 <- knn(train=train.st[,-16], test=valid.st[,-16], cl=train.st[,16], k=1)
#knn10 <- knn(train=train.st[,-37], test=valid.st[,-37], cl=train.st[,37], k=10)
knn1.test <- knn(train=train.st[,-16], test=test.st[,-16], cl=train.st[,16], k=1)
#knn10.test <- knn(train=train.st[,-37], test=test.st[,-37], cl=train.st[,37], k=10)
knn1.acc <- data.frame(df=c("valid","test"))
knn1.acc$Balanced_Accuracy <- c(confusionMatrix(knn1, valid.st[,16], positive = "1")$byClass["Balanced Accuracy"], confusionMatrix(knn1.test, test.st[,16], positive = "1")$byClass["Balanced Accuracy"])
knn1.acc$F1 <- c(confusionMatrix(knn1, valid.st[,16], positive = "1")$byClass["F1"], confusionMatrix(knn1.test, test.st[,16], positive = "1")$byClass["F1"])
# knn.acc <- data.frame(df=c("valid k=1","valid k=10","test k=1","test k=10"))
# knn.acc$Balanced_Accuracy <- c(confusionMatrix(knn1, valid.st[,37], positive = "1")$byClass["F1"], confusionMatrix(knn10, valid.st[,37], positive = "1")$byClass["F1"], confusionMatrix(knn1.test, test.st[,37], positive = "1")$byClass["F1"], confusionMatrix(knn10.test, test.st[,37], positive = "1")$byClass["F1"])
# knn.acc$F1 <- c(confusionMatrix(knn1, valid.st[,37], positive = "1")$byClass["F1"], confusionMatrix(knn10, valid.st[,37], positive = "1")$byClass["F1"], confusionMatrix(knn1.test, test.st[,37], positive = "1")$byClass["F1"], confusionMatrix(knn10.test, test.st[,37], positive = "1")$byClass["F1"])
library(reshape)
knn.melt <- melt(knn1.acc, id="df")
ggplot(data=knn.melt, aes(x=df, y=value, colour=variable))+
geom_col(position="dodge", fill="grey96")+
theme_bw()
View(knn1.acc)
View(df.dummy)
