---
title: "Project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
df <- read.csv(file = "C:/Users/leomi/Downloads/Data-mining/Project_CVTDM/onlinedeliverydata.csv", header = T, sep = ",")
#setwd("C:/Users/andre/Desktop/Creating Value Project")
#df <- read.csv("onlinedeliverydata.csv", header = TRUE, sep = ",")
df <- df[,-c(8,9,10)]
str(df)
attach(df)
```

## Recoding variables

```{r}
library(dplyr)

df1 <- df[,-52]

for (i in c(14:33,37:41)) {
  df1[,i] <- recode(df1[,i], "Strongly Agree"=5, "Agree"=4,"Neutral"=3,"Disagree"=2,"Strongly Disagree"=1)
}

library(naniar)
gg_miss_var(df1[,c(14:33,37:41)], show_pct = TRUE)

levels(df[,51])
df1$Output <- recode(df1$Output, "Yes"=1, "No"=0)
gg_miss_var(df[,50:51], show_pct = TRUE)

levels(as.factor(df1[,5]))
df1$Monthly.Income=recode(df$Monthly.Income, "10001 to 25000"=17500.5, "25001 to 50000"=37500, "Below Rs.10000" = 10000, "More than 50000"=50000, "No Income"=0 )

levels(df[,43])
str(df[,43:50])

for (i in c(43:50)) {
  df1[,i] <- recode(df1[,i], "Important"=4, "Moderately Important"=3, "Slightly Important"=2, "Unimportant"=1, "Very Important"=5)
}
gg_miss_var(df1[,43:50], show_pct = TRUE)

levels(df[,34])
str(df[,c(34,42)])
for (i in c(34,42)) {
  df1[,i] <- recode(df1[,i], "Maybe"=2, "No"=1, "Yes"=3)
}
gg_miss_var(df1[,c(34,42)], show_pct = TRUE)

levels(df[,2])
df1$Gender <- recode(df1$Gender, "Female"=1, "Male"=0)
gg_miss_var(df1[,1:2], show_pct = TRUE)



df1=df1[,-c(8:13)]
str(df1)
```

## Data frames

```{r}
df2 <- df1[,c(1,3,4,5,6,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,27,30,36,38,39,40,41,45)]
str(df2)
```

## Dummies

```{r}
df2$Marital.Status =as.factor(df2$Marital.Status)
df2$Occupation=as.factor(df2$Occupation)
df2$Educational.Qualifications=as.factor(df2$Educational.Qualifications)

library(caret)
dummies=dummyVars(~., data=df2)
df.dummy=as.data.frame(predict(dummies, newdata=df2))

df.dummy=df.dummy[,-c(3,6,14)]
str(df.dummy)
```

```{r}
df.dummy$Output <- as.factor(df.dummy$Output)
df2$Output <- as.factor(df2$Output)
```

## Partitioning

```{r}
library(caret)

set.seed(1)
train_rows <- createDataPartition(df2$Output, p = .5, list = FALSE)
train <- df2[train_rows,]
valid_test <- df2[-train_rows,]
set.seed(1)
valid_rows <- createDataPartition(valid_test$Output, p = .6, list = FALSE)
valid <- valid_test[valid_rows,]
test <- valid_test[-valid_rows,]
```

```{r}
set.seed(1)
train_rows1 <- createDataPartition(df.dummy$Output, p = .5, list = FALSE)
train.dummy <- df.dummy[train_rows1,]
valid_test.dummy <- df.dummy[-train_rows1,]
set.seed(1)
valid_rows1 <- createDataPartition(valid_test.dummy$Output, p = .6, list = FALSE)
valid.dummy <- valid_test.dummy[valid_rows1,]
test.dummy <- valid_test.dummy[-valid_rows1,]
```


```{r}
print(table(train$Output) / table(df2$Output)) #Training set partition
print(table(valid$Output) / table(df2$Output)) #Validation set partition
print(table(test$Output) / table(df2$Output)) #Test set partition
```

## Discriminant analysis

```{r}
library(MASS)
library(DiscriMiner)
```


```{r}
#da1.reg <- linDA(train[,1:30], train[,31])
#da1.reg
#View(train)
```

```{r}
model1 <- lda(Output~., data = train)
model1
```


```{r}
pred.valid=predict(model1,valid[,-31])

cm.da.valid=confusionMatrix(as.factor(pred.valid$class),as.factor(valid$Output) ,positive = "1")
cm.da.valid
```

```{r}
fourfoldplot(cm.da.valid$table)
```

```{r}
da.pred.test=predict(model1,test[,-31])

cm.da.test=confusionMatrix(as.factor(da.pred.test$class),as.factor(test$Output) ,positive = "1")
cm.da.test
```

```{r}
fourfoldplot(cm.da.test$table)
```

## Neural Nets

```{r}
library(neuralnet)
library(caret)
library(nnet)
library(NeuralNetTools)
library(boot)
library(plyr)
library(ggplot2)
library(reshape)
```


```{r}
train.dummy.scale=train.dummy
valid.dummy.scale=valid.dummy
test.dummy.scale=test.dummy

vars = c(1,7,12:36)
norm.values <- preProcess(train.dummy[, vars], method="range") 

train.dummy.scale[,vars] = predict(norm.values, train.dummy[,vars])
valid.dummy.scale[,vars] = predict(norm.values, valid.dummy[,vars])
test.dummy.scale[,vars] = predict(norm.values, test.dummy[,vars])

scale_back_min_max_score <- function(x_) {
  min_x = min(train.dummy$Output)
  max_x = max(train.dummy$Output)
  x = sapply(x_, function(x__) x__*(max_x - min_x) + min_x)
  return(x)
}
```

#### 1 layer 5 nodes
```{r}
set.seed(1)

names(train.dummy.scale)[names(train.dummy.scale) == "Occupation.Self Employeed"] <- "Occupation.SelfEmployeed"
names(train.dummy.scale)[names(train.dummy.scale) == "Educational.Qualifications.Post Graduate"] <- "Educational.Qualifications.PostGraduate"


NN_1_5 <- neuralnet(Output ~. , train.dummy.scale, linear.output = FALSE, hidden = 5)

#head(prediction(NN_1_5))
plot(NN_1_5)
```

```{r}
library(caret)
nn1.predict.valid <-  neuralnet::compute(NN_1_5, valid.dummy.scale[,-37])
predicted.class=apply(nn1.predict.valid$net.result,1,which.max)-1

confusionMatrix(as.factor(predicted.class), valid.dummy.scale$Output, positive = "1")
```

```{r}
nn1.predict.test <-  neuralnet::compute(NN_1_5, test.dummy.scale[,-37])
predicted.class=apply(nn1.predict.test$net.result,1,which.max)-1
#str(test.dummy.scale)

confusionMatrix(as.factor(predicted.class), test.dummy.scale$Output, positive = "1")
```
#### 5 lalyers 5 nodes

```{r}
set.seed(1)

names(train.dummy.scale)[names(train.dummy.scale) == "Occupation.Self Employeed"] <- "Occupation.SelfEmployeed"
names(train.dummy.scale)[names(train.dummy.scale) == "Educational.Qualifications.Post Graduate"] <- "Educational.Qualifications.PostGraduate"


NN_5_5 <- neuralnet(Output ~. , train.dummy.scale, linear.output = FALSE, hidden = c(5,5,5,5,5))

#head(prediction(NN_1_2))
plot(NN_5_5)
```

```{r}
library(caret)
nn1.predict.valid <-  neuralnet::compute(NN_5_5, valid.dummy.scale[,-37])
predicted.class=apply(nn1.predict.valid$net.result,1,which.max)-1

confusionMatrix(as.factor(predicted.class), valid.dummy.scale$Output, positive = "1")
```

```{r}
nn1.predict.test <-  neuralnet::compute(NN_5_5, test.dummy.scale[,-37])
predicted.class=apply(nn1.predict.test$net.result,1,which.max)-1
#str(test.dummy.scale)
confusionMatrix(as.factor(predicted.class), test.dummy.scale$Output, positive = "1")
```

## Classification Tree

#### Full-grown tree
```{r}
library(forecast)
library(caret)
library(Metrics)
library(dplyr)
library(reshape2)
library(scales)
library(rpart)

set.seed(1)
full_CT = rpart(Output ~ ., data = train, method = "class", cp=0, minbucket=1, minsplit = 1, xval =5)
length(full_CT$frame$var[full_CT$frame$var == "<leaf>"])
```

```{r}
library(ggplot2) 
library(cowplot)
library(rpart.plot)


rpart.plot(full_CT, type = 1, extra = 1, under = TRUE, split.font = 1, varlen = -10)
```

```{r}
row_minxerr=which.min(full_CT$cptable[,4])
cp_minxerr=full_CT$cptable[row_minxerr,1]
cp_minxerr
```
```{r}
minxerr=full_CT$cptable[row_minxerr,4]
xstd_minxerr=full_CT$cptable[row_minxerr,5]
cp_best_pruned_CT=minxerr+xstd_minxerr
cp_best_pruned_CT
```
```{r}
printcp(full_CT)[,4]
```
```{r}
cp_xerr=full_CT$cptable[5,1]
cp_xerr
```
```{r}
plotcp(full_CT) + 
abline(v = row_minxerr, lty = "dashed")+
abline(v = 5, lty = "dotted")
```

Letâ€™s prune 2 classification trees: first one with CP of 1 split(which has the lowest cross-validation error) and second with CP of 7 splits(best-pruned classification tree)

#### Pruned classification tree
```{r}
set.seed(1)
pruned_CT <- prune(full_CT, cp = cp_minxerr)
length(pruned_CT$frame$var[pruned_CT$frame$var == "<leaf>"])
```
```{r}
rpart.plot(pruned_CT, type = 1, extra = 1, under = TRUE, split.font = 1, varlen = -10)
```

```{r}
pred_valid_pruned_CT= predict(pruned_CT, valid, type="class")

valid$Output=as.factor(valid$Output)
CT_cm_valid=confusionMatrix(pred_valid_pruned_CT, valid$Output , positive = "1")
CT_cm_valid
```

```{r}
fourfoldplot(confusionMatrix(pred_valid_pruned_CT, valid$Output , positive = "1")$table)
```


```{r}
pred_test_pruned_CT= predict(pruned_CT, test, type="class")

test$Output=as.factor(test$Output)
CT_cm_valid=confusionMatrix(pred_test_pruned_CT, test$Output , positive = "1")
CT_cm_valid
```

```{r}
fourfoldplot(confusionMatrix(pred_test_pruned_CT, test$Output , positive = "1")$table)
```


#### Best pruned classification tree

```{r}
best_pruned_CT <- prune(full_CT,cp = cp_xerr)
length(best_pruned_CT$frame$var[best_pruned_CT$frame$var == "<leaf>"])
```

```{r}
rpart.plot(best_pruned_CT, type = 1, extra = 1, under = TRUE, split.font = 1, varlen = -10)
```

```{r}
pred_valid_best_pruned_CT= predict(best_pruned_CT, valid, type="class")

valid$Output=as.factor(valid$Output)
CT_cm_valid=confusionMatrix(pred_valid_best_pruned_CT, valid$Output , positive = "1")
CT_cm_valid
```

```{r}
fourfoldplot(confusionMatrix(pred_valid_best_pruned_CT, valid$Output , positive = "1")$table)
```


```{r}
pred_test_best_pruned_CT= predict(best_pruned_CT, test, type="class")

test$Output=as.factor(test$Output)
CT_cm_valid=confusionMatrix(pred_test_best_pruned_CT, test$Output , positive = "1")
CT_cm_valid
```

```{r}
fourfoldplot(confusionMatrix(pred_test_best_pruned_CT, test$Output , positive = "1")$table)
```

## Boosted

```{r}
library(cowplot)
library(caret)
library(rpart)
library(gains)
library(adabag)
library(randomForest)
library(reshape2)
```


```{r}
set.seed(1)
boost <- boosting(Output ~ ., data = train)
```


```{r}
pred_boost_valid <- predict(boost, valid)
boost_valid.cm = confusionMatrix(as.factor(pred_boost_valid$class), valid$Output, positive = "1")

boost_valid.cm
```

```{r}
pred_boost_test <- predict(boost, test)
boost_test.cm = confusionMatrix(as.factor(pred_boost_test$class), (test$Output),positive = "1")

boost_test.cm
```

## Bagged

```{r}
set.seed(1)
bagt <- bagging(Output ~ ., data = train)
```


```{r}
pred <- predict(bagt, valid)
bag_valid.cm = confusionMatrix(as.factor(pred$class), valid$Output,positive = "1")

bag_valid.cm
```

```{r}
pred <- predict(bagt, test)
bag_test.cm = confusionMatrix(as.factor(pred$class), test$Output,positive = "1")

bag_test.cm
```

## Random

```{r}
set.seed(1)
rf <- randomForest(Output ~ ., data = train, mtry=4, importance = T)
```


```{r}
pred <- predict(rf, valid)

rf_valid.cm = confusionMatrix(as.factor(pred), valid$Output,positive = "1")

rf_valid.cm
```

```{r}
pred <- predict(rf, test)

rf_valid.cm = confusionMatrix(as.factor(pred), test$Output, positive = "1")

rf_valid.cm
```
