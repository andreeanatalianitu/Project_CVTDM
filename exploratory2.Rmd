---
title: 'Project'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
#df <- read.csv(file = "C:/Users/leomi/Downloads/Data-mining/Project_CVTDM/onlinedeliverydata.csv", header = T, sep = ",")
setwd("C:/Users/andre/Desktop/Creating Value Project")
df <- read.csv("onlinedeliverydata.csv", header = TRUE, sep = ",")
df <- df[,-c(8,9,10)]
str(df)
attach(df)
View(df)
```


```{r}
library(reshape2)
subset.class.category=df[,c(2:6,14:50)]

for (i in c(2:6,8:50)) {

explore1=df[,c(i, 51)]
explore1=melt(explore1, id="Output")
explore.cast=dcast(explore1, Output~value, fun.aggregate=length)
explore2=melt(explore.cast, id="Output")

explore2$Output=as.factor(explore2$Output)

print(ggplot(explore2) + 
  scale_fill_brewer(palette="Pastel2")+
  geom_col(aes(x=explore2$variable, y=explore2$value, fill=explore2$Output),  position = "dodge")+
  labs(title=names(df)[c(i)], x="", y="value",fill="Output:",color=""))+
  theme_minimal()
}
```


```{r}
library(dplyr)

df1 <- df[,-52]

str(df[,c(14:33,37:41)])

for (i in c(14:33,37:41)) {
  df1[,i] <- recode(df1[,i], "Strongly Agree"=5, "Agree"=4,"Neutral"=3,"Disagree"=2,"Strongly Disagree"=1)
}

library(naniar)
gg_miss_var(df1[,c(14:33,37:41)], show_pct = TRUE)

levels(df[,51])
df1$Output <- recode(df1$Output, "Yes"=1, "No"=0)
gg_miss_var(df[,50:51], show_pct = TRUE)

levels(as.factor(df1[,5]))
df1$Monthly.Income=recode(df$Monthly.Income, "10001 to 25000"=17500.5, "25001 to 50000"=37500, "Below Rs.10000" = 10000, "More than 50000"=50000, "No Income"=0 )

levels(df[,43])
str(df[,43:50])

for (i in c(43:50)) {
  df1[,i] <- recode(df1[,i], "Important"=4, "Moderately Important"=3, "Slightly Important"=2, "Unimportant"=1, "Very Important"=5)
}
gg_miss_var(df1[,43:50], show_pct = TRUE)


levels(df[,35])


levels(df[,34])
str(df[,c(34,42)])
for (i in c(34,42)) {
  df1[,i] <- recode(df1[,i], "Maybe"=2, "No"=1, "Yes"=3)
}
gg_miss_var(df1[,c(34,42)], show_pct = TRUE)

levels(df[,12])
str(df[,c(12:13)])
levels(df[,11])
levels(df[,10])
levels(df[,9])
levels(df[,8])
levels(df[,6])
levels(df[,5])
levels(df[,4])
levels(df[,3])

levels(df[,2])
df1$Gender <- recode(df1$Gender, "Female"=1, "Male"=0)
gg_miss_var(df1[,1:2], show_pct = TRUE)


str(df[,c(1,7,36)])

```



```{r}
#str(df1)
#df1$Gender <- as.factor(df1$Gender)
#df1$Output <- as.factor(df1$Output)
df1=df1[,-c(8:13)]
str(df1)

```

```{r fig.height=6, fig.width=6}
numeric.df=df1[,-c(3,4,6,29)]
library(ggplot2)
library(ggcorrplot)
corr <- round(cor(numeric.df,method="spearman"), 3)
ggcorrplot(corr, hc.order = TRUE,
   outline.col = "white",
   ggtheme = ggplot2::theme_gray,
   colors = c("#6D9EC1", "white", "#E46726"))
```
### VIF
```{r}
VIF = function(matx, y){
  #
  fit.dat = data.frame(y=y, matx)
  fit.lm = lm(y ~ . , data=fit.dat)
  #
  vifs = vif.lm.car(fit.lm)
  #
  cat("\n")
  print(signif(vifs))
  cat("\n  Mean:", format(signif(mean(vifs))), "\n\n")
}
vif.lm.car = function(mod, ...) 
{
  # Calculates variance-inflation and generalized variance-inflation factors for linear and generalized linear models. 
  #
  # "vif" function from package "car" (Version 2.0-12)
  # Source: http://cran.r-project.org/web/packages/car/
  # R documentation: ?car::vif
  if (any(is.na(coef(mod))))
    stop("there are aliased coefficients in the model")
  v <- vcov(mod)
  assign <- attributes(model.matrix(mod))$assign
  if (names(coefficients(mod)[1]) == "(Intercept)") {
    v <- v[-1, -1]
    assign <- assign[-1]
  }
  else warning("No intercept: vifs may not be sensible.")
  terms <- labels(terms(mod))
  n.terms <- length(terms)
  if (n.terms < 2) 
    stop("model contains fewer than 2 terms")
  R <- cov2cor(v)
  detR <- det(R)
  result <- matrix(0, n.terms, 3)
  rownames(result) <- terms
  colnames(result) <- c("GVIF", "Df", "GVIF^(1/(2*Df))")
  for (term in 1:n.terms) {
    subs <- which(assign == term)
    result[term, 1] <- det(as.matrix(R[subs, subs])) * det(as.matrix(R[-subs, -subs]))/detR
    result[term, 2] <- length(subs)
  }
  if (all(result[, 2] == 1)) 
    result <- result[, 1]
  else result[, 3] <- result[, 1]^(1/(2 * result[, 2]))
  result
}

VIF(numeric.df[,-41], numeric.df[,41])
```

### DA

```{r}
df1$Marital.Status =as.factor(df1$Marital.Status)
df1$Occupation=as.factor(df1$Occupation)
df1$Educational.Qualifications=as.factor(df1$Educational.Qualifications)
df1$Order.Time=as.factor(df1$Order.Time)
library(caret)
dummies=dummyVars(~., data=df1)
df_dummy=as.data.frame(predict(dummies, newdata=df1))

df_dummy=df_dummy[,-c(4,7,15,38)]
str(df_dummy)
```


```{r}
library(gridExtra)
library(ggplot2)

boxplot_exp<-ggplot(df_dummy) +
  geom_boxplot(aes(x=Age), fill="white") +
  labs(title="Range of Age values")+
  theme_minimal()+
  theme(plot.title = element_text(face="bold"))
median=median(df_dummy[,1])
mean=mean(df_dummy[,1])
hist_exp<-ggplot(df_dummy) +
  geom_histogram(aes(x=Age), bins=10, fill = "slategray1",color="grey") +
  geom_vline (aes(xintercept=median,col="median"))+
  geom_vline(aes(xintercept=mean,col="mean"))+
  labs(title="Distribution of Age",color="")+
  theme_minimal()+
  theme(plot.title = element_text(face="bold"))
grid.arrange(boxplot_exp, hist_exp, ncol=2)


```

```{r}
boxplot_exp<-ggplot(df_dummy) +
  geom_boxplot(aes(x=Monthly.Income), fill="white") +
  labs(title="Range of Age values")+
  theme_minimal()+
  theme(plot.title = element_text(face="bold"))
median=median(df_dummy[,8])
mean=mean(df_dummy[,8])
hist_exp<-ggplot(df_dummy) +
  geom_histogram(aes(x=Monthly.Income), bins=10, fill = "slategray1",color="grey") +
  geom_vline (aes(xintercept=median,col="median"))+
  geom_vline(aes(xintercept=mean,col="mean"))+
  labs(title="Distribution of Age",color="")+
  theme_minimal()+
  theme(plot.title = element_text(face="bold"))
grid.arrange(boxplot_exp, hist_exp, ncol=2)
```

```{r}
boxplot_exp<-ggplot(df_dummy) +
  geom_boxplot(aes(x=Maximum.wait.time ), fill="white") +
  labs(title="Range of Age values")+
  theme_minimal()+
  theme(plot.title = element_text(face="bold"))
median=median(df_dummy[,37])
mean=mean(df_dummy[,37])
hist_exp<-ggplot(df_dummy) +
  geom_histogram(aes(x=Maximum.wait.time), bins=10, fill = "slategray1",color="grey") +
  geom_vline (aes(xintercept=median,col="median"))+
  geom_vline(aes(xintercept=mean,col="mean"))+
  labs(title="Distribution of Age",color="")+
  theme_minimal()+
  theme(plot.title = element_text(face="bold"))
grid.arrange(boxplot_exp, hist_exp, ncol=2)
```

```{r}
library(caret)
norm.values.da = preProcess(df_dummy[,c(1,8,37)], method=c("center","scale"))
norm.da.df = predict(norm.values.da, df_dummy)

library(DiscriMiner)
library(MASS)
model1 <- lda(Output~., data = norm.da.df)
model1

library(corrr)
#corr(norm.da.df)
#ggcorrplot(corr, hc.order = TRUE, outline.col = "white",ggtheme = ggplot2::theme_gray,colors = c("#6D9EC1", "white", "#E46726"))
str(df_dummy)

norm.da.df$Output=as.factor(norm.da.df$Output)
var.selct.df1 <- linDA(norm.da.df[,-c(52)], norm.da.df[,52])
scores=var.selct.df1$functions 
score=as.data.frame(scores)
colnames(score)=c("No.score","Yes.score")
attach(score)
new.score=score[order(-Yes.score),]
new.score
detach(score)
```



```{r}
library(caret)
norm.values.da = preProcess(df_dummy[,-c(2:7,9:12,35,36,52)], method=c("center","scale"))
norm.da.df = predict(norm.values.da, df_dummy)

library(DiscriMiner)
library(MASS)
model1 <- lda(Output~., data = norm.da.df)
model1

library(corrr)
#corr(norm.da.df)
#ggcorrplot(corr, hc.order = TRUE, outline.col = "white",ggtheme = ggplot2::theme_gray,colors = c("#6D9EC1", "white", "#E46726"))


norm.da.df$Output=as.factor(norm.da.df$Output)
var.selct.df1 <- linDA(norm.da.df[,-c(52)], norm.da.df[,52])
scores=var.selct.df1$functions 
score=as.data.frame(scores)
colnames(score)=c("No.score","Yes.score")
attach(score)
new.score=score[order(-Yes.score),]
new.score
detach(score)
```


### Logistic regression

Next steps will be setting seed, partitioning the data into training(50%), validation(30%) and test(20%) datasets.

```{r}
logit.df=df1
for (i in c(3:4,6,8:13,35)){
  logit.df[,i]=as.factor(logit.df[,i])
}
str(logit.df)

set.seed(1)

train.index <- sample(rownames(logit.df), dim(logit.df)[1]*0.5)
valid.index <- sample(setdiff(rownames(logit.df), train.index), dim(logit.df)[1]*0.3)
test.index <- setdiff(rownames(logit.df), union(train.index, valid.index))

train.df <- logit.df[train.index, ]
valid.df <- logit.df[valid.index, ]
test.df <- logit.df[test.index, ]
```



```{r}
logit_reg <- glm(Output ~ ., data = train.df, family = "binomial")
options(scipen=999)
summary(logit_reg)
```

```{r}
forward <- step(logit_reg,direction ="forward")
summary(forward)
```


```{r}
df1$Output <- as.factor(df1$Output)
str(df1)
variables <- as.data.frame(colnames(df1)) 
```


### Classifcation tree

```{r fig.height=9, fig.width=10}
library(rpart)
library(rpart.plot)
tree <- rpart(Output ~., data=df1, method="class", cp=0, minsplit=1, minbucket=1)
prp(tree, cex=1)
rpart.plot(tree, cex=0.8)
df_classtree <- df1[,c(1,2,3,5,6,8,9,10,14,15,19,21,22,24,25,26,27,31,33,34,36,28,40,45)]
```


### Random Forest

```{r}
library(randomForest)
rf <- randomForest(Output ~., data=df1, ntree = 500, mtry = 4, nodesize = 5, importance = TRUE)
varImpPlot(rf, type = 1, cex=0.7)
df_randomf <- df1[,c(1,3,4,5,6,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,27,30,36,38,39,40,41,45)]
```


```{r}
df_da <- df1[,c(1,2,3,4,6,9,10,12,15,16,17,19,21,22,23,26,27,29,30,31,33,34,36,37,38,40,42,43,45)]
```


### Naive Bayes

#### Data partitioning

```{r}
prop.table(table(df1$Output))

set.seed(1)
train.rows <- createDataPartition(df1$Output, p = .6, list = FALSE)

train1 <- df_classtree[train.rows,]
valid1 <- df_classtree[-train.rows,]

train2 <- df_randomf[train.rows,]
valid2 <- df_randomf[-train.rows,]

train3 <- df_da[train.rows,]
valid3 <- df_da[-train.rows,]

print(table(train1$Output) / table(df1$Output)) #Training set partition
print(table(valid1$Output) / table(df1$Output)) #Validation set partition
```

```{r}
library(e1071)

naivebayes1 <- naiveBayes(Output ~., data=train1)
pred1 <- predict(naivebayes1, newdata = valid1)

naivebayes2 <- naiveBayes(Output ~., data=train2)
pred2 <- predict(naivebayes2, newdata = valid2)

naivebayes3 <- naiveBayes(Output ~., data=train3)
pred3 <- predict(naivebayes3, newdata = valid3)

accuracy <- data.frame(model = c("class tree", "random forest", "discriminant analysis"))

accuracy$Balanced_Accuracy <- c(confusionMatrix(pred1, valid1$Output)$byClass["Balanced Accuracy"], confusionMatrix(pred2, valid2$Output)$byClass["Balanced Accuracy"], confusionMatrix(pred3, valid3$Output)$byClass["Balanced Accuracy"])

accuracy$F1 <- c(confusionMatrix(pred1, valid1$Output)$byClass["F1"], confusionMatrix(pred2, valid2$Output)$byClass["F1"], confusionMatrix(pred3, valid3$Output)$byClass["F1"])
```

## Logistic regression

```{r}
logit1 <- glm(Output ~ ., data = train1, family = "binomial")
#summary(logit1)
log.pred1 = predict(logit1, valid1[,-45], type = "response")


logit2 <- glm(Output ~ ., data = train2, family = "binomial")
#summary(logit2)
log.pred2 = predict(logit2, valid2[,-45], type = "response")

logit3 <- glm(Output ~ ., data = train3, family = "binomial")
#summary(logit3)
log.pred3 = predict(logit3, valid3[,-45], type = "response")

accuracy1 <- data.frame(model=c("class tree", "random forest", "discriminant analysis"))

accuracy1$Balanced_Accuracy <- c(confusionMatrix(as.factor(ifelse(log.pred1 > 0.5, 1, 0)), valid1$Output)$byClass["Balanced Accuracy"], confusionMatrix(as.factor(ifelse(log.pred2 > 0.5, 1, 0)), valid2$Output)$byClass["Balanced Accuracy"], confusionMatrix(as.factor(ifelse(log.pred3 > 0.5, 1, 0)), valid3$Output)$byClass["Balanced Accuracy"])

accuracy1$F1 <- c(confusionMatrix(as.factor(ifelse(log.pred1 > 0.5, 1, 0)), valid1$Output)$byClass["F1"], confusionMatrix(as.factor(ifelse(log.pred2 > 0.5, 1, 0)), valid2$Output)$byClass["F1"], confusionMatrix(as.factor(ifelse(log.pred3 > 0.5, 1, 0)), valid3$Output)$byClass["F1"])
```

## Classification Tree

```{r}
set.seed(1)
tree1 <- rpart(Output ~., data=train1, method="class", cp=0, minsplit=1, minbucket=1, xval=5)
pruned1 <- prune(tree1, cp = tree1$cptable[which.min(tree1$cptable[,"xerror"]),"CP"])
tree.pred1 <- predict(pruned1, valid1, type="class")

set.seed(1)
tree2 <- rpart(Output ~., data=train2, method="class", cp=0, minsplit=1, minbucket=1, xval=5)
pruned2 <- prune(tree2, cp = tree2$cptable[which.min(tree2$cptable[,"xerror"]),"CP"])
tree.pred2 <- predict(pruned2, valid2, type="class")

set.seed(1)
tree3 <- rpart(Output ~., data=train3, method="class", cp=0, minsplit=1, minbucket=1, xval=5)
pruned3 <- prune(tree3, cp = tree3$cptable[which.min(tree3$cptable[,"xerror"]),"CP"])
tree.pred3 <- predict(pruned3, valid3, type="class")

accuracy2 <- data.frame(model=c("class tree", "random forest", "discriminant analysis"))

accuracy2$Balanced_Accuracy <- c(confusionMatrix(tree.pred1, valid1$Output, positive="1")$byClass["Balanced Accuracy"], confusionMatrix(tree.pred2, valid2$Output, positive="1")$byClass["Balanced Accuracy"], confusionMatrix(tree.pred3, valid3$Output, positive="1")$byClass["Balanced Accuracy"])

accuracy2$F1 <- c(confusionMatrix(tree.pred1, valid1$Output, positive="1")$byClass["F1"], confusionMatrix(tree.pred2, valid2$Output, positive="1")$byClass["F1"], confusionMatrix(tree.pred3, valid3$Output, positive="1")$byClass["F1"])

```
















